{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# ARTI308 - Lab 4 Assignment Solution\n",
    "## Data Quality Assessment & Preprocessing\n",
    "\n",
    "This notebook completes all 5 assignment tasks from Lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Load and prepare the dataset (same preprocessing as in the lab)\n",
    "df = pd.read_csv(\"Chocolate_Sales.csv\")\n",
    "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "df['Amount'] = df['Amount'].replace(r'[\\$,]', '', regex=True)\n",
    "df['Amount'] = pd.to_numeric(df['Amount'])\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1_title",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1: Identify Data Quality Issues\n",
    "\n",
    "In this task, we systematically identify all data quality issues in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_dtypes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a. Check original data types BEFORE cleaning\n",
    "# Load the raw file to see original issues\n",
    "df_raw = pd.read_csv(\"Chocolate_Sales.csv\")\n",
    "\n",
    "print(\"=== ORIGINAL DATA TYPES (before any cleaning) ===\")\n",
    "print(df_raw.dtypes)\n",
    "print()\n",
    "print(\"Sample of raw data:\")\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1_issues",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Identify all data quality issues\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"   DATA QUALITY ISSUES IDENTIFIED\")\n",
    "print(\"========================================\")\n",
    "\n",
    "# Issue 1: Wrong data type for 'Date'\n",
    "print(\"\\n--- ISSUE 1: Incorrect Data Type for 'Date' ---\")\n",
    "print(f\"  Current type: {df_raw['Date'].dtype}  (should be datetime64)\")\n",
    "print(f\"  Example value: '{df_raw['Date'].iloc[0]}'\")\n",
    "print(\"  Impact: Cannot perform time-based analysis (sorting, filtering by date, etc.)\")\n",
    "\n",
    "# Issue 2: Wrong data type for 'Amount'\n",
    "print(\"\\n--- ISSUE 2: Incorrect Data Type for 'Amount' ---\")\n",
    "print(f\"  Current type: {df_raw['Amount'].dtype}  (should be float64 or int64)\")\n",
    "print(f\"  Example value: '{df_raw['Amount'].iloc[0]}'\")\n",
    "print(\"  Reason: Contains '$' and ',' characters, making it a string (object)\")\n",
    "print(\"  Impact: Cannot perform arithmetic operations or statistical analysis\")\n",
    "\n",
    "# Issue 3: Missing values\n",
    "print(\"\\n--- ISSUE 3: Missing Values ---\")\n",
    "missing = df_raw.isna().sum()\n",
    "print(missing)\n",
    "if missing.sum() == 0:\n",
    "    print(\"  ✓ No missing values found in the original dataset.\")\n",
    "else:\n",
    "    print(f\"  ⚠ Total missing values: {missing.sum()}\")\n",
    "\n",
    "# Issue 4: Check for duplicate rows\n",
    "print(\"\\n--- ISSUE 4: Duplicate Rows ---\")\n",
    "duplicates = df_raw.duplicated().sum()\n",
    "print(f\"  Number of duplicate rows: {duplicates}\")\n",
    "if duplicates == 0:\n",
    "    print(\"  ✓ No duplicate rows found.\")\n",
    "\n",
    "# Issue 5: Check for outliers in numeric columns\n",
    "print(\"\\n--- ISSUE 5: Potential Outliers (using cleaned numeric data) ---\")\n",
    "for col in ['Amount', 'Boxes Shipped']:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outlier_count = df[(df[col] < lower) | (df[col] > upper)].shape[0]\n",
    "    print(f\"  '{col}': {outlier_count} outliers detected (outside [{lower:.2f}, {upper:.2f}])\")\n",
    "\n",
    "print(\"\\n========================================\")\n",
    "print(\"SUMMARY: 2 type errors, 0 missing values, outliers present\")\n",
    "print(\"========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task2_title",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Apply One Missing Value Strategy and Explain Why\n",
    "\n",
    "Since the original dataset has no missing values, we introduce artificial ones (as done in the lab) and then apply a strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_introduce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Introduce artificial missing values (same as lab)\n",
    "df_missing = df.copy()\n",
    "df_missing.loc[0:5, 'Amount'] = np.nan\n",
    "\n",
    "print(\"Missing values after introducing NaN:\")\n",
    "print(df_missing.isna().sum())\n",
    "print(f\"\\nRows with missing Amount: {df_missing['Amount'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize the distribution to decide which strategy is best\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogram to check distribution shape\n",
    "df['Amount'].hist(bins=30, ax=axes[0], color='steelblue', edgecolor='white')\n",
    "axes[0].set_title('Distribution of Amount')\n",
    "axes[0].set_xlabel('Amount')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Boxplot to check for skewness/outliers\n",
    "df.boxplot(column='Amount', ax=axes[1])\n",
    "axes[1].set_title('Boxplot of Amount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean   of Amount: {df['Amount'].mean():.2f}\")\n",
    "print(f\"Median of Amount: {df['Amount'].median():.2f}\")\n",
    "print(f\"Skewness: {df['Amount'].skew():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task2_apply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply MEDIAN Imputation (chosen strategy)\n",
    "\n",
    "df_task2 = df_missing.copy()\n",
    "median_value = df_task2['Amount'].median()\n",
    "df_task2['Amount'].fillna(median_value, inplace=True)\n",
    "\n",
    "print(\"=== CHOSEN STRATEGY: Median Imputation ===\")\n",
    "print()\n",
    "print(\"JUSTIFICATION:\")\n",
    "print(\"  1. The distribution of 'Amount' is RIGHT-SKEWED (skewness > 0),\")\n",
    "print(\"     meaning there are high-value transactions pulling the mean upward.\")\n",
    "print(\"  2. The MEAN is sensitive to these outliers and would OVERESTIMATE\")\n",
    "print(\"     the typical transaction value when used for imputation.\")\n",
    "print(\"  3. The MEDIAN represents the middle value and is ROBUST to outliers,\")\n",
    "print(\"     providing a more realistic replacement for missing values.\")\n",
    "print(\"  4. With only 6 missing values out of 3282 rows (0.18%), imputation\")\n",
    "print(\"     is preferred over deletion to preserve all data.\")\n",
    "print()\n",
    "print(f\"  Median value used for imputation: {median_value:.2f}\")\n",
    "print()\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df_task2.isna().sum())\n",
    "print()\n",
    "print(\"First 7 rows (rows 0-5 had NaN, now filled with median):\")\n",
    "df_task2.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3_title",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Detect and Handle Outliers Using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_detect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Detect outliers using the IQR method for BOTH numeric columns\n",
    "\n",
    "print(\"=== OUTLIER DETECTION USING IQR ===\")\n",
    "print()\n",
    "\n",
    "results = {}\n",
    "for col in ['Amount', 'Boxes Shipped']:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower) | (df[col] > upper)]\n",
    "    results[col] = {'Q1': Q1, 'Q3': Q3, 'IQR': IQR,\n",
    "                    'lower': lower, 'upper': upper,\n",
    "                    'count': len(outliers), 'data': outliers}\n",
    "    \n",
    "    print(f\"Column: '{col}'\")\n",
    "    print(f\"  Q1 = {Q1:.2f}\")\n",
    "    print(f\"  Q3 = {Q3:.2f}\")\n",
    "    print(f\"  IQR = Q3 - Q1 = {IQR:.2f}\")\n",
    "    print(f\"  Lower fence = Q1 - 1.5*IQR = {lower:.2f}\")\n",
    "    print(f\"  Upper fence = Q3 + 1.5*IQR = {upper:.2f}\")\n",
    "    print(f\"  Outliers detected: {len(outliers)} rows\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Visualize outliers using boxplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for i, col in enumerate(['Amount', 'Boxes Shipped']):\n",
    "    sns.boxplot(y=df[col], ax=axes[i], color='lightcoral')\n",
    "    axes[i].set_title(f'Boxplot of {col}\\n(dots = outliers)')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Outlier rows for 'Amount':\")\n",
    "results['Amount']['data'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3_handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Handle outliers — Applying CAPPING (Winsorization) on 'Amount'\n",
    "\n",
    "df_task3 = df.copy()\n",
    "\n",
    "# Recalculate bounds for Amount\n",
    "Q1_a = df['Amount'].quantile(0.25)\n",
    "Q3_a = df['Amount'].quantile(0.75)\n",
    "IQR_a = Q3_a - Q1_a\n",
    "lower_a = Q1_a - 1.5 * IQR_a\n",
    "upper_a = Q3_a + 1.5 * IQR_a\n",
    "\n",
    "# Cap values at the IQR fences\n",
    "df_task3['Amount'] = df_task3['Amount'].clip(lower=lower_a, upper=upper_a)\n",
    "\n",
    "print(\"=== HANDLING STRATEGY: Capping (Winsorization) ===\")\n",
    "print()\n",
    "print(\"JUSTIFICATION:\")\n",
    "print(\"  - Capping replaces extreme values with the fence values instead of removing rows.\")\n",
    "print(\"  - This preserves dataset size (3282 rows unchanged).\")\n",
    "print(\"  - Extreme sales transactions may represent real bulk/corporate orders —\")\n",
    "print(\"    it's safer to cap them than delete them entirely.\")\n",
    "print(\"  - Unlike deletion, capping retains all other information in those rows.\")\n",
    "print()\n",
    "print(f\"Original dataset shape:    {df.shape}\")\n",
    "print(f\"After capping shape:       {df_task3.shape}  (no rows removed!)\")\n",
    "print()\n",
    "print(f\"Amount - max before capping: {df['Amount'].max():.2f}\")\n",
    "print(f\"Amount - max after  capping: {df_task3['Amount'].max():.2f}  (= upper fence)\")\n",
    "\n",
    "# Verify outliers are gone\n",
    "remaining = df_task3[(df_task3['Amount'] < lower_a) | (df_task3['Amount'] > upper_a)].shape[0]\n",
    "print(f\"\\nOutliers remaining in 'Amount' after capping: {remaining}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4_title",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Normalize Numerical Features Using Both Min-Max and Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_minmax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Min-Max Normalization\n",
    "numeric_cols = ['Amount', 'Boxes Shipped']\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_minmax = df[numeric_cols].copy()\n",
    "df_minmax[numeric_cols] = minmax_scaler.fit_transform(df_minmax)\n",
    "\n",
    "print(\"=== MIN-MAX NORMALIZATION ===\")\n",
    "print()\n",
    "print(\"Formula: X_scaled = (X - X_min) / (X_max - X_min)\")\n",
    "print(\"Result range: [0, 1] for all features\")\n",
    "print()\n",
    "print(\"Original data (first 5 rows):\")\n",
    "print(df[numeric_cols].head())\n",
    "print()\n",
    "print(\"After Min-Max normalization (first 5 rows):\")\n",
    "print(df_minmax.head())\n",
    "print()\n",
    "print(\"Verification — Min and Max after scaling:\")\n",
    "print(df_minmax.agg(['min', 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_zscore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Z-Score Standardization\n",
    "standard_scaler = StandardScaler()\n",
    "df_zscore = df[numeric_cols].copy()\n",
    "df_zscore[numeric_cols] = standard_scaler.fit_transform(df_zscore)\n",
    "\n",
    "print(\"=== Z-SCORE STANDARDIZATION ===\")\n",
    "print()\n",
    "print(\"Formula: Z = (X - mean) / std_deviation\")\n",
    "print(\"Result: mean ≈ 0, std ≈ 1 for all features\")\n",
    "print()\n",
    "print(\"After Z-Score standardization (first 5 rows):\")\n",
    "print(df_zscore.head())\n",
    "print()\n",
    "print(\"Verification — Mean and Std after scaling (should be ~0 and ~1):\")\n",
    "print(df_zscore.agg(['mean', 'std']).round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4_compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Visual comparison of all three\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "\n",
    "dfs = [df, df_minmax, df_zscore]\n",
    "titles = ['Original', 'Min-Max Normalized', 'Z-Score Standardized']\n",
    "\n",
    "for row, (data, title) in enumerate(zip(dfs, titles)):\n",
    "    for col_idx, col in enumerate(numeric_cols):\n",
    "        axes[row][col_idx].hist(data[col], bins=30, color='steelblue', edgecolor='white')\n",
    "        axes[row][col_idx].set_title(f'{title} — {col}')\n",
    "        axes[row][col_idx].set_xlabel('Value')\n",
    "        axes[row][col_idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"  - Shape of distribution is the SAME across all three versions.\")\n",
    "print(\"  - Only the SCALE on the x-axis changes.\")\n",
    "print(\"  - Min-Max → values range from 0 to 1.\")\n",
    "print(\"  - Z-Score → values centered around 0, spread across negative and positive.\")\n",
    "print()\n",
    "print(\"WHEN TO USE WHICH:\")\n",
    "print(\"  Min-Max: Best for KNN, K-Means, Neural Networks (needs bounded range [0,1]).\")\n",
    "print(\"  Z-Score: Best for Linear Regression, SVM, PCA (assumes centered data).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5_title",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Apply PCA and Interpret Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5_apply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply PCA on Z-Score standardized data (PCA requires standardized features)\n",
    "X = df_zscore[numeric_cols]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X)\n",
    "\n",
    "print(\"=== PCA RESULTS ===\")\n",
    "print()\n",
    "print(\"Explained Variance Ratio per component:\")\n",
    "for i, var in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {var:.6f}  ({var*100:.4f}%)\")\n",
    "\n",
    "print()\n",
    "print(f\"Total variance explained by both PCs: {sum(pca.explained_variance_ratio_)*100:.4f}%\")\n",
    "print()\n",
    "print(\"Component loadings (eigenvectors):\")\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_,\n",
    "    columns=numeric_cols,\n",
    "    index=['PC1', 'PC2']\n",
    ")\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5_scree",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scree plot — how much variance each PC captures\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(['PC1', 'PC2'],\n",
    "            pca.explained_variance_ratio_ * 100,\n",
    "            color=['steelblue', 'lightcoral'], edgecolor='white')\n",
    "axes[0].set_title('Scree Plot: Variance Explained per PC')\n",
    "axes[0].set_ylabel('Variance Explained (%)')\n",
    "axes[0].set_ylim(0, 100)\n",
    "for i, v in enumerate(pca.explained_variance_ratio_ * 100):\n",
    "    axes[0].text(i, v + 1, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# PCA Scatter plot\n",
    "axes[1].scatter(principal_components[:, 0], principal_components[:, 1],\n",
    "                alpha=0.3, s=10, color='steelblue')\n",
    "axes[1].set_title('PCA Projection (All Transactions)')\n",
    "axes[1].set_xlabel('Principal Component 1')\n",
    "axes[1].set_ylabel('Principal Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5_interpret",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Full interpretation\n",
    "var_pc1 = pca.explained_variance_ratio_[0] * 100\n",
    "var_pc2 = pca.explained_variance_ratio_[1] * 100\n",
    "\n",
    "print(\"=== PCA INTERPRETATION ===\")\n",
    "print()\n",
    "print(f\"PC1 explains {var_pc1:.2f}% of the total variance.\")\n",
    "print(f\"PC2 explains {var_pc2:.2f}% of the total variance.\")\n",
    "print()\n",
    "print(\"KEY FINDING:\")\n",
    "print(f\"  The variance is split nearly equally between PC1 ({var_pc1:.1f}%) and\")\n",
    "print(f\"  PC2 ({var_pc2:.1f}%). Neither component dominates the other.\")\n",
    "print()\n",
    "print(\"WHAT THIS MEANS:\")\n",
    "print(\"  - If PC1 captured 85%+ variance, we could reduce 2 features → 1 (PC1 only).\")\n",
    "print(\"  - Here, both components are needed to retain most information.\")\n",
    "print(\"  - This confirms the finding from the correlation heatmap in the lab:\")\n",
    "print(\"    'Amount' and 'Boxes Shipped' have near-ZERO correlation (~-0.013).\")\n",
    "print(\"  - Since the features are INDEPENDENT (not correlated), PCA cannot\")\n",
    "print(\"    compress them into fewer meaningful components without losing information.\")\n",
    "print()\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"  PCA would NOT be beneficial for dimensionality reduction on this dataset.\")\n",
    "print(\"  It is applied here for DEMONSTRATION purposes only, as stated in the lab.\")\n",
    "print(\"  PCA is most useful when features are strongly correlated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_title",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment Summary\n",
    "\n",
    "| Task | Description | Key Finding |\n",
    "|------|-------------|-------------|\n",
    "| **Task 1** | Identify data quality issues | 2 type errors: `Date` stored as string, `Amount` has `$` symbols; outliers present |\n",
    "| **Task 2** | Apply missing value strategy | **Median imputation** chosen — dataset is right-skewed, median is robust to outliers |\n",
    "| **Task 3** | Detect & handle outliers | IQR detected outliers in `Amount`; handled by **capping** to preserve all rows |\n",
    "| **Task 4** | Normalize features | Both **Min-Max** (→ [0,1]) and **Z-Score** (→ mean=0, std=1) applied |\n",
    "| **Task 5** | PCA + interpretation | ~50/50 variance split — PCA NOT useful here due to near-zero correlation between features |\n",
    "\n",
    "---\n",
    "*End of Lab 4 Assignment Solution*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
